{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "import itertools\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.mlab as mlab\n",
    "\n",
    "import autograd.numpy as np\n",
    "from autograd import grad\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define a function to perform [AdaM](https://arxiv.org/abs/1412.6980) updates..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# AdaM: Adaptive Moments Optimizer\n",
    "## Params\n",
    "### alpha0: base learning rate\n",
    "### grad: current gradient\n",
    "### adam_values: dictionary containing moment estimates\n",
    "\n",
    "def get_AdaM_update(alpha_0, grad, adam_values, b1=.95, b2=.999, e=1e-8):\n",
    "    adam_values['t'] += 1\n",
    "\n",
    "    # update mean                                                                                                                                                                                                     \n",
    "    adam_values['mean'] = b1 * adam_values['mean'] + (1-b1) * grad\n",
    "    m_hat = adam_values['mean'] / (1-b1**adam_values['t'])\n",
    "\n",
    "    # update variance                                                                                                                                                                                                 \n",
    "    adam_values['var'] = b2 * adam_values['var'] + (1-b2) * grad**2\n",
    "    v_hat = adam_values['var'] / (1-b2**adam_values['t'])\n",
    "\n",
    "    return alpha_0 * m_hat/(np.sqrt(v_hat) + e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.  Simulated Posterior: Mixture Density\n",
    "\n",
    "We are going to demonstrate all techniques on a simulated posterior.  As posteriors in the wild are often multimodal, we'll use a Gaussian mixture as our demonstration..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Gaussian\n",
    "def gaussPdf(x, params):\n",
    "    # params: {'mu': mean, 'sigma': standard dev.}\n",
    "    return (1./np.sqrt(2*np.pi*params['sigma']**2)) * np.exp((-.5/params['sigma']**2) * np.sum((x-params['mu'])**2))\n",
    "\n",
    "# 2D Gaussian Mixture\n",
    "def logGaussMixPDF(x, params):\n",
    "    # params: {'pi': list of weights, 'mu': list of means, 'sigma': list of standard devs}\n",
    "    return np.log(params['pi'][0] * gaussPdf(x, {'mu':params['mu'][0], 'sigma':params['sigma'][0]}) \\\n",
    "            + params['pi'][1] * gaussPdf(x, {'mu':params['mu'][1], 'sigma':params['sigma'][1]}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's choose the parameters and plot the density..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "true_posterior_params = {\n",
    "    'mu': [-4,3],\n",
    "    'sigma': [1, 3],\n",
    "    'pi': [.3, .7]\n",
    "}\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "theta_grid = np.linspace(-10, 10, 1000)\n",
    "\n",
    "probs_true = [np.exp(logGaussMixPDF(z, true_posterior_params)) for z in theta_grid]\n",
    "plt.plot(theta_grid, probs_true, 'b-', linewidth=5, label=\"True Posterior\")\n",
    "\n",
    "plt.xlabel(r\"$\\theta$\")\n",
    "plt.xlim([-10,10])\n",
    "plt.ylim([0,.25])\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.  Variational Inference via the ELBO\n",
    "\n",
    "Now let's fit a Gaussian approximation--$q(\\boldsymbol{\\theta};\\boldsymbol{\\phi}) \\ = \\ \\text{N}(\\boldsymbol{\\mu}_{\\boldsymbol{\\phi}}, \\boldsymbol{\\sigma}_{\\boldsymbol{\\phi}})$--using the [*Evidence Lower Bound* (ELBO)](http://www.umiacs.umd.edu/~xyang35/files/understanding-variational-lower.pdf): $$ \\log p(x) = \\mathbb{E}_{q(\\boldsymbol{\\theta})}[ \\log p(\\mathbf{X}, \\boldsymbol{\\theta})] + \\mathbb{H}_{q}[\\boldsymbol{\\theta}] + \\text{KLD}[q(\\boldsymbol{\\theta}) || p(\\boldsymbol{\\theta} | \\mathbf{X})] \\ge \\mathbb{E}_{q(\\boldsymbol{\\theta})}[ \\log p(\\mathbf{X}, \\boldsymbol{\\theta})] + \\mathbb{H}_{q}[\\boldsymbol{\\theta}].$$\n",
    "\n",
    "We'll perform fully gradient-based optimization.  Differentiating we have... $$ \\frac{\\partial}{\\partial \\boldsymbol{\\phi}} \\mathcal{L}_{\\text{ELBO}} =  \\frac{\\partial}{\\partial \\boldsymbol{\\phi}} \\mathbb{E}_{q(\\boldsymbol{\\theta})}[ \\log p(\\mathbf{X}, \\boldsymbol{\\theta})] + \\frac{\\partial}{\\partial \\boldsymbol{\\phi}} \\mathbb{H}_{q}[\\boldsymbol{\\theta}].$$\n",
    "\n",
    "Furthermore, so that we don't have to compute the above expectation by hand, we'll use a Monte Carlo (MC) approximation: $$\\mathbb{E}_{q(\\boldsymbol{\\theta})}[ \\log p(\\mathbf{X}, \\boldsymbol{\\theta})] \\approx \\frac{1}{S} \\sum_{s} \\log p(\\mathbf{X}, \\hat{\\boldsymbol{\\theta}}_{s})  $$ where $\\hat{\\boldsymbol{\\theta}}_{s} \\sim q(\\boldsymbol{\\theta})$.  The resulting gradient is then $$ \\frac{\\partial}{\\partial \\boldsymbol{\\phi}} \\frac{1}{S} \\sum_{s} \\log p(\\mathbf{X}, \\hat{\\boldsymbol{\\theta}}_{s}) = \\frac{1}{S} \\sum_{s} \\frac{\\partial}{\\partial \\hat{\\boldsymbol{\\theta}}_{s}} \\log p(\\mathbf{X}, \\hat{\\boldsymbol{\\theta}}_{s}) \\frac{\\partial \\hat{\\boldsymbol{\\theta}}_{s}}{\\partial \\boldsymbol{\\phi}}.$$  More importantly, we'll also use MC techniques when the expectations don't have analytical solutions (like in the case of Bayesian neural networks).\n",
    "\n",
    "The first step will be to get the necessary gradient terms via Autograd..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Entropy of Gaussian\n",
    "def gaussEntropy(log_sigma):\n",
    "    return .5 * (np.log(2*np.pi*np.e) + 2.*log_sigma)\n",
    "\n",
    "# Function for sampling from Gaussian location-scale form\n",
    "def sample_from_Gauss(mu, log_sigma):\n",
    "    e = np.random.normal()\n",
    "    return mu + np.exp(log_sigma) * e, e\n",
    "\n",
    "\n",
    "### GET DERIVATIVES ###\n",
    "\n",
    "# d log p(X, \\theta) / d \\theta\n",
    "logModel = logGaussMixPDF\n",
    "dLogModel_dTheta = grad(logModel)\n",
    "\n",
    "# d theta / d log_sigma\n",
    "### we'll implement this ourselves\n",
    "\n",
    "# d entropy / d log_sigma\n",
    "dEntropy_dLogSigma = grad(gaussEntropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets initialize the variational parameters and optimize the ELBO..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### INIT VARIATIONAL PARAMS \n",
    "phi = {'mu':-5., 'log_sigma':0.}\n",
    "\n",
    "\n",
    "### ELBO OPTIMIZATION\n",
    "maxEpochs = 500\n",
    "learning_rate = .1\n",
    "adam_values = {'mu':{'mean': 0., 'var': 0., 't': 0}, 'log_sigma':{'mean': 0., 'var': 0., 't': 0}}\n",
    "n_samples = 10\n",
    "\n",
    "for epochIdx in range(maxEpochs):\n",
    "    \n",
    "    elbo_grad_mu, elbo_grad_log_sigma = 0., 0.\n",
    "    for s in range(n_samples):\n",
    "        \n",
    "        theta_hat, rand_seed = sample_from_Gauss(phi['mu'], phi['log_sigma'])\n",
    "        dModel_dTheta = dLogModel_dTheta(theta_hat, true_posterior_params)\n",
    "        \n",
    "        elbo_grad_mu += 1./n_samples * dModel_dTheta * 1.\n",
    "        elbo_grad_log_sigma += 1./n_samples * dModel_dTheta * rand_seed * np.exp(phi['log_sigma'])\n",
    "        \n",
    "    elbo_grad_log_sigma += dEntropy_dLogSigma(phi['log_sigma'])\n",
    "        \n",
    "    phi['mu'] += get_AdaM_update(learning_rate, elbo_grad_mu, adam_values['mu'])  \n",
    "    phi['log_sigma'] += get_AdaM_update(learning_rate, elbo_grad_log_sigma, adam_values['log_sigma']) \n",
    "        \n",
    "print phi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize the results..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "probs_approx = [gaussPdf(z, {'mu':phi['mu'], 'sigma':np.exp(phi['log_sigma'])}) for z in theta_grid] \n",
    "    \n",
    "plt.figure()\n",
    "\n",
    "plt.plot(theta_grid, probs_true, 'b-', linewidth=7, label=\"True Posterior\")\n",
    "plt.plot(theta_grid, probs_approx, '-r', linewidth=5, label=\"Variational Approx.\")\n",
    "\n",
    "plt.xlabel(r\"$\\theta$\")\n",
    "plt.xlim([-10,10])\n",
    "plt.ylim([0,.25])\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.  Variational Inference via Stein Variational Gradient Descent (SVGD)\n",
    "##### by Qiang Liu, Dilin Wang (https://arxiv.org/abs/1608.04471)\n",
    "\n",
    "Particles explore the posterior according to the following iterative algorithm:\n",
    "$$ \\boldsymbol{\\theta}_{k}^{t+1} \\leftarrow \\boldsymbol{\\theta}_{k}^{t} + \\epsilon \\ \\hat{\\boldsymbol{\\psi}}[\\boldsymbol{\\theta}_{k}^{t}] \\  \\text{  where  } \\  \\hat{\\boldsymbol{\\psi}}[\\cdot] = \\frac{1}{K} \\sum_{j=1}^{K} k(\\boldsymbol{\\theta}_{j}, \\boldsymbol{\\theta}_{k}) \\nabla_{\\boldsymbol{\\theta}_{j}} \\log p(\\mathbf{X}, \\boldsymbol{\\theta}_{j}) + \\nabla_{\\boldsymbol{\\theta}_{j}} k(\\boldsymbol{\\theta}_{j}, \\boldsymbol{\\theta}_{k})$$\n",
    "where $k(\\cdot, \\cdot)$ is a valid kernel and $\\epsilon$ is a step-size.  Let's implement a kernel and the operator $\\hat{\\boldsymbol{\\psi}}[\\boldsymbol{\\theta}_{k}^{t}]$ below..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Kernel: Radial Basis Function \n",
    "def rbf(x1, x2, params={'lengthScale': 5.}):\n",
    "    return np.exp((-.5/params['lengthScale']) * np.sum((x1-x2)**2))\n",
    "\n",
    "\n",
    "# SVGD Operator \n",
    "def steinOp(theta_particles, dLogModel, kernel, dKernel):\n",
    "    K = len(theta_particles)\n",
    "    \n",
    "    # precompute model derivative w.r.t. each particle\n",
    "    dModel_dThetas = [0.] * K\n",
    "    for k in range(K):\n",
    "        dModel_dThetas[k] = dLogModel(theta_particles[k], true_posterior_params)\n",
    "    \n",
    "    # compute each particle's update\n",
    "    particle_updates = [0.] * K\n",
    "    for k in range(K):\n",
    "        for j in range(K):\n",
    "            particle_updates[k] += kernel(theta_particles[j], theta_particles[k]) * dModel_dThetas[j] \\\n",
    "                                    + dKernel(theta_particles[j], theta_particles[k])\n",
    "        particle_updates[k] /= K\n",
    "        \n",
    "    return particle_updates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets initialize the particles and optimize..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### INIT VARIATIONAL PARTICLES \n",
    "n_particles = 10\n",
    "theta_particles = [np.random.normal(loc=-5.) for k in range(n_particles)]\n",
    "\n",
    "\n",
    "### STEIN VARIATIONAL GRADIENT DESCENT\n",
    "maxEpochs = 500\n",
    "learning_rate = .1\n",
    "adam_values = [{'mean': 0., 'var': 0., 't': 0} for k in range(n_particles)]\n",
    "\n",
    "for epochIdx in range(maxEpochs):\n",
    "    \n",
    "    particle_updates = steinOp(theta_particles, dLogModel_dTheta, rbf, grad(rbf))\n",
    "    \n",
    "    for k in range(n_particles):\n",
    "        theta_particles[k] += get_AdaM_update(learning_rate, particle_updates[k], adam_values[k])  \n",
    "        \n",
    "print theta_particles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize the results..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "theta_particles.sort()\n",
    "probs_approx = [np.exp(logModel(z, true_posterior_params)) for z in theta_particles]   \n",
    "    \n",
    "plt.figure()\n",
    "\n",
    "plt.plot(theta_grid, probs_true, 'b-', linewidth=7, label=\"True Posterior\")\n",
    "plt.plot(theta_particles, probs_approx, 'sr-', markersize=14, mew=0, linewidth=5, label=\"Variational Approx.\")\n",
    "\n",
    "plt.xlabel(r\"$\\theta$\")\n",
    "plt.xlim([-10,10])\n",
    "plt.ylim([0,.25])\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.  Variational Inference via Stein Mixtures\n",
    "\n",
    "Lastly, we now propose a variational inference algorithm that combines the ELBO (#2) and SVGD (#3).  Essentailly, we are going to run SVGD on the parameters of the variational approximation instead of on the original generative model parameter space ($\\boldsymbol{\\theta}$).  Mathematically, the *variational* model can be specified as: $$ \\boldsymbol{\\phi} \\sim \\frac{1}{K} \\sum_{k} \\delta[\\boldsymbol{\\phi}_{k}], \\ \\ \\ \\boldsymbol{\\theta} \\sim q(\\boldsymbol{\\theta} | \\boldsymbol{\\phi}) \\ \\ \\text{ thus making the marginal posterior a mixture of the form } \\ \\  q(\\boldsymbol{\\theta}) = \\frac{1}{K} \\sum_{k} q(\\boldsymbol{\\theta} | \\boldsymbol{\\phi}_{k}). $$  The corresponding Stein update is then defined as: $$ \\boldsymbol{\\phi}_{k}^{t+1} \\leftarrow \\boldsymbol{\\phi}_{k}^{t} + \\epsilon \\ \\hat{\\boldsymbol{\\psi}}[\\boldsymbol{\\phi}_{k}^{t}] \\  \\text{  where  } \\  \\hat{\\boldsymbol{\\psi}}[\\cdot] = \\frac{1}{K} \\sum_{j=1}^{K} k(\\boldsymbol{\\phi}_{j}, \\boldsymbol{\\phi}_{k}) \\sum_{s} \\tilde w_{s} \\nabla_{\\boldsymbol{\\phi}_{j}} \\log \\frac{p(\\mathbf{X}, \\hat{\\boldsymbol{\\theta}}_{s})}{q(\\hat{\\boldsymbol{\\theta}}_{s} | \\boldsymbol{\\phi}_{j})} + \\nabla_{\\boldsymbol{\\phi}_{j}} k(\\boldsymbol{\\phi}_{j}, \\boldsymbol{\\phi}_{k}) \\ \\ \\text{ and } \\ \\ \\tilde w_{s} = \\frac{p(\\mathbf{X}, \\hat{\\boldsymbol{\\theta}}_{s})/q(\\hat{\\boldsymbol{\\theta}}_{s} | \\boldsymbol{\\phi}_{j})}{\\sum_{l} p(\\mathbf{X}, \\hat{\\boldsymbol{\\theta}}_{l})/q(\\hat{\\boldsymbol{\\theta}}_{l} | \\boldsymbol{\\phi}_{j})}$$\n",
    "and, again, where $k(\\cdot, \\cdot)$ is a valid kernel and $\\epsilon$ is a step-size.  Notice when $S=1$, the objective simplifies to a one-sample MC estimate of the ELBO.  However, notice that when all terms (both model and entropy) in the ELBO are sampled, the gradient becomes slightly more complex: <img src=\"./graphics/sELBO_gradient.png\" alt=\"Drawing\" style=\"width: 600px;\"/>  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll need to get some more derivative terms for the kernel and $q(\\hat{\\theta})$..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Kernel: Probability Product Kernel \n",
    "# http://www.jmlr.org/papers/volume5/jebara04a/jebara04a.pdf\n",
    "def prob_prod(z1_mu, z1_log_sigma, z2_mu, z2_log_sigma, rho=1.75):\n",
    "    \n",
    "    z1_sigma = np.exp(2*z1_log_sigma)\n",
    "    z2_sigma = np.exp(2*z2_log_sigma)\n",
    "    sigma_star = 1./z1_sigma + 1./z2_sigma\n",
    "    mu_star = z1_mu/z1_sigma + z2_mu/z2_sigma\n",
    "\n",
    "    return np.exp( -rho/2. * ((z1_mu**2)/z1_sigma + (z2_mu**2)/z2_sigma - (mu_star**2)/sigma_star ) )\n",
    "\n",
    "\n",
    "### GET KERNEL DERIVATIVES\n",
    "kernel_grad_fns = {}\n",
    "kernel_grad_fns['mu'] = grad(prob_prod)\n",
    "kernel_grad_fns['log_sigma'] = grad(lambda log_sigma1, mu1, mu2, log_sigma2: prob_prod(mu1, log_sigma1, mu2, log_sigma2))\n",
    "\n",
    "### GET Q(\\theta) DERIVATIVES\n",
    "logQ_grad_fns = {}\n",
    "logQ_grad_fns['x'] = grad(lambda x, mu, log_sigma: np.log(gaussPdf(x, {'mu':mu, 'sigma':np.exp(log_sigma)})))\n",
    "logQ_grad_fns['mu'] = grad(lambda mu, x, log_sigma: np.log(gaussPdf(x, {'mu':mu, 'sigma':np.exp(log_sigma)})))\n",
    "logQ_grad_fns['log_sigma'] = grad(lambda log_sigma, x, mu: np.log(gaussPdf(x, {'mu':mu, 'sigma':np.exp(log_sigma)})))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can implement the operator $\\hat{\\boldsymbol{\\psi}}[\\boldsymbol{\\theta}_{k}^{t}]$ below, assuming $S=1$..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Stein Mixture Operator \n",
    "def steinMixOp(phi_particles, dLogModel, dLogQ, kernel, dKernel):\n",
    "    K = len(phi_particles)\n",
    "    \n",
    "    # precompute model derivative w.r.t. each particle\n",
    "    # assumes just ONE sample is taken\n",
    "    grad_mu, grad_logSig = [0.]*K, [0.]*K\n",
    "    for k in range(K):\n",
    "        \n",
    "        theta_hat, rand_seed = sample_from_Gauss(phi_particles[k]['mu'], phi_particles[k]['log_sigma'])\n",
    "        dModel_dTheta = dLogModel(theta_hat, true_posterior_params)\n",
    "        dTheta_dLogSig = rand_seed * np.exp(phi_particles[k]['log_sigma'])\n",
    "        \n",
    "        grad_mu[k] += dModel_dTheta * 1. \n",
    "        grad_mu[k] += -dLogQ['x'](theta_hat, phi_particles[k]['mu'], phi_particles[k]['log_sigma']) * 1\n",
    "        grad_mu[k] += -dLogQ['mu'](phi_particles[k]['mu'], theta_hat, phi_particles[k]['log_sigma'])\n",
    "        \n",
    "        grad_logSig[k] += dModel_dTheta * dTheta_dLogSig \n",
    "        grad_logSig[k] += -dLogQ['x'](theta_hat, phi_particles[k]['mu'], phi_particles[k]['log_sigma']) * dTheta_dLogSig\n",
    "        grad_logSig[k] += -dLogQ['log_sigma'](phi_particles[k]['log_sigma'], theta_hat, phi_particles[k]['mu'])\n",
    "        \n",
    "        \n",
    "    # compute each particle's update\n",
    "    particle_updates_mu, particle_updates_logSig = [0.]*K, [0.]*K \n",
    "    for k in range(K):\n",
    "        \n",
    "        mu_k, logSig_k = phi_particles[k]['mu'], phi_particles[k]['log_sigma']\n",
    "        for j in range(K):\n",
    "            \n",
    "            mu_j, logSig_j = phi_particles[j]['mu'], phi_particles[j]['log_sigma']\n",
    "            \n",
    "            particle_updates_mu[k] += kernel(mu_j, logSig_j, mu_k, logSig_k) * grad_mu[j] \\\n",
    "                                    + dKernel['mu'](mu_j, logSig_j, mu_k, logSig_k)\n",
    "            particle_updates_logSig[k] += kernel(mu_j, logSig_j, mu_k, logSig_k) * grad_logSig[j] \\\n",
    "                                    + dKernel['log_sigma'](logSig_j, mu_j, mu_k, logSig_k)\n",
    "                \n",
    "        particle_updates_mu[k] /= K\n",
    "        particle_updates_logSig[k] /= K\n",
    "        \n",
    "    return particle_updates_mu, particle_updates_logSig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets initialize the particles and optimize..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### INIT VARIATIONAL PARTICLES \n",
    "n_particles = 3\n",
    "phi_particles = [{'mu':np.random.normal(), 'log_sigma':0.} for k in range(n_particles)]\n",
    "\n",
    "\n",
    "### STEIN MIXTURES\n",
    "maxEpochs = 500\n",
    "mu_learning_rate = .03\n",
    "logSig_learning_rate = .003\n",
    "adam_values = [{'mu':{'mean': 0., 'var': 0., 't': 0}, 'log_sigma':{'mean': 0., 'var': 0., 't': 0}} for k in range(n_particles)]\n",
    "\n",
    "for epochIdx in range(maxEpochs):\n",
    "    \n",
    "    particle_updates = steinMixOp(phi_particles, dLogModel_dTheta, logQ_grad_fns, prob_prod, kernel_grad_fns)\n",
    "    \n",
    "    for k in range(n_particles):\n",
    "        phi_particles[k]['mu'] += get_AdaM_update(mu_learning_rate, particle_updates[0][k], adam_values[k]['mu'])\n",
    "        phi_particles[k]['log_sigma'] += get_AdaM_update(logSig_learning_rate, particle_updates[0][k], adam_values[k]['log_sigma'])\n",
    "        \n",
    "print phi_particles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets visualize the approximation..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "\n",
    "plt.plot(theta_grid, probs_true, 'b-', linewidth=7, label=\"True Posterior\")\n",
    "\n",
    "probs_approx = []\n",
    "for k in range(n_particles):\n",
    "    probs_approx.append([gaussPdf(z, {'mu':phi_particles[k]['mu'], 'sigma':np.exp(phi_particles[k]['log_sigma'])}) for z in theta_grid]) \n",
    "    plt.plot(theta_grid, probs_approx[-1], '--k', linewidth=2, label=\"Component #%d\" %(k+1))\n",
    "    \n",
    "full_approx = 1./n_particles * np.array(probs_approx).sum(axis=0)\n",
    "plt.plot(theta_grid, full_approx, '-r', linewidth=5, label=\"Full Approximation\")\n",
    "\n",
    "plt.xlabel(r\"$\\theta$\")\n",
    "plt.xlim([-10,10])\n",
    "plt.ylim([0,.25])\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
